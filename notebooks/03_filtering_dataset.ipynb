{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas\n",
    "import pandas as pd\n",
    "# viewing options\n",
    "pd.set_option('max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "gender\n",
       "boy     235804\n",
       "girl    174119\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# import dataframe, drop duplicates\n",
    "df = pd.read_csv('./data/enron/03_processed_body.csv', index_col=0)\n",
    "df.groupby(['gender']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring impact on label\n",
    "We want to keep an eye on the male/female ratio when filtering out our dataset (especially when blanket removing things like 'duplicates', where the filtering can be applied to either label for the same condition) so that our label ratio isn't significantly unbalanced as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Size: 409923, B/G Ratio: 1.354\n"
     ]
    }
   ],
   "source": [
    "def monitor_label(df):\n",
    "    b = df.groupby(['gender']).size().boy\n",
    "    g = df.groupby(['gender']).size().girl\n",
    "    print('Frame Size: {}, B/G Ratio: {:.3f}'.format(b+g, b/g))\n",
    "    \n",
    "monitor_label(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove NaN from gender, email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Size: 409719, B/G Ratio: 1.354\n"
     ]
    }
   ],
   "source": [
    "# set dataframe to not NaN gender returns\n",
    "df = df[df.gender.notna()]\n",
    "\n",
    "# return index of NaN email body\n",
    "_ = df[df.p_body.isna()].index\n",
    "\n",
    "# drop NaN body\n",
    "df = df.drop(_)\n",
    "\n",
    "monitor_label(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate emails from body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Size: 182823, B/G Ratio: 1.315\n"
     ]
    }
   ],
   "source": [
    "# set dataframe to dropped duplicates\n",
    "df = df.drop_duplicates('p_body')\n",
    "\n",
    "monitor_label(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset index\n",
    "Prior to using the indices and indexes to reference data in the dataframe, we'll need to reset the index so the values line up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex dataframe for cosine similarity matching\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity (cossim) to find near-duplicate items\n",
    "Here we'll use cosine similarity scores across samples from our population to review for filtering criteria. Because we're working with limited memory, we'll take a sample from the dataframe, transform it with tfidf, and calculate the cossim score.    \n",
    "\n",
    "Once the cossim matrix is returned for all documents in the corpus, we'll get a list of the indices where cossim score is greater than ~90-95% and filter the dataframe by indexing the indice returns. **This will return a subset of the sample with high cossim.**    \n",
    "\n",
    "Finally, we'll fit the data with a Multinomial Naive Bayes model to identify features from the dataset for review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Cosine Similarity (cossim) to evaluate identical email body items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# import numpy\n",
    "import numpy as np\n",
    "\n",
    "# viewing options\n",
    "np.set_printoptions(edgeitems=10)\n",
    "np.core.arrayprint._line_width = 300\n",
    "\n",
    "# user function\n",
    "def cos_sim_this(df, p=0.95, n=10000):\n",
    "    \"\"\"input a dataframe, cos sim %, and sample number\n",
    "       return a filtered dataframe of the cos sim results\"\"\"\n",
    "    # reset index (needs full, linear index or will throw flag)\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # collect a sample\n",
    "    sample = df.sample(n, random_state=42)\n",
    "\n",
    "    # feature extraction - tfidf\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    tf_vect = TfidfVectorizer()\n",
    "    tfidf = tf_vect.fit_transform(sample.p_body)\n",
    "\n",
    "    # pairwise - cosine similarity\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    cos = cosine_similarity(tfidf) # return pairwise similarities between all samples in input (Note: can be sliced [0:1000])\n",
    "\n",
    "    # make a list of array indices that match our cos % condition for dataframe indexing\n",
    "    idx_list = []\n",
    "    for doc_ind in np.arange(len(cos)):\n",
    "        doc_tup = np.where(cos[doc_ind] > p)\n",
    "        if len(doc_tup[0]) > 1:\n",
    "            x = doc_tup[0]\n",
    "            idx_list.append(x[x != doc_ind][0])\n",
    "    idx_list = list(set(idx_list))\n",
    "\n",
    "    # get high cossim selection from original df (by index)\n",
    "    filter_df = df.loc[idx_list, :]\n",
    "    \n",
    "    return filter_df\n",
    "\n",
    "\n",
    "# call function\n",
    "initial_sample = cos_sim_this(df, p=0.9, n=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a model to identify features for filtering\n",
    "\n",
    "### Vectorize input data\n",
    "We'll use a Tfidf vectorizer to transform the text here because we're interested in identifying 'features' across our subsample with high cosine similarity to identify any keywords that can help with filtering out mass almost-perfect duplicates that would skew our dataset (ex. FW:, RE:, spam, etc).\n",
    "\n",
    "### Reviewing 'forwarded by'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "(coef score, % of df has feature, feature string)\n",
      "\n",
      "-9.593 12% hou ect ect cc\n",
      "-9.741 9% ect ect cc subject\n",
      "-10.152 4% eric bass hou ect\n",
      "-10.289 5% sally beck hou ect\n",
      "-10.316 3% bass hou ect ect\n",
      "-10.380 4% enron north america corp\n",
      "-10.404 3% john arnold hou ect\n",
      "-10.410 4% enron com cc subject\n",
      "-10.433 2% wireless handheld www blackberry\n",
      "-10.433 0% sent blackberry wireless handheld\n",
      "-10.433 2% handheld www blackberry net\n",
      "-10.433 2% blackberry wireless handheld www\n",
      "-10.482 0% original message arnold john\n",
      "-10.489 4% hou ect ect subject\n",
      "-10.489 2% arnold hou ect ect\n",
      "-10.494 0% message arnold john sent\n",
      "-10.497 2% phillip allen hou ect\n",
      "-10.508 4% beck hou ect ect\n",
      "-10.610 2% eric bass enron com\n",
      "-10.661 3% thanks lynn original message\n",
      "Wall time: 1.92 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create stopwords list\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# set stopwords list to english\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def check_feature_perc(s, a_df):\n",
    "    ft_len = len(a_df[a_df.p_body.str.contains(s)])\n",
    "    df_len = len(a_df)\n",
    "    return (ft_len/df_len)*100\n",
    "\n",
    "\n",
    "def get_features(df, n=10, ngrams=(1,1), transform='t', stop_words=stop_words):\n",
    "    # get variables for model\n",
    "    Xf = df.p_body.values\n",
    "    yf = np.zeros((len(Xf)), np.int8) # dumbie label '0'\n",
    "\n",
    "    # instantiate, fit transformer\n",
    "    if transform == 't':\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        tf_vect = TfidfVectorizer(analyzer='word', ngram_range=ngrams, stop_words=stop_words)\n",
    "        X_t = tf_vect.fit_transform(Xf)\n",
    "        v = tf_vect\n",
    "    \n",
    "    # instantiate, fit vectorizer\n",
    "    if transform == 'c':\n",
    "        from sklearn.feature_extraction.text import CountVectorizer\n",
    "        c_vect = CountVectorizer(analyzer='word', ngram_range=ngrams, stop_words=stop_words)\n",
    "        X_t = c_vect.fit_transform(Xf)\n",
    "        v = c_vect\n",
    "\n",
    "    # import model\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "    # instantiate model object\n",
    "    mnb = MultinomialNB()\n",
    "\n",
    "    # fit model on data\n",
    "    mnb.fit(X_t, yf)\n",
    "\n",
    "    # return features by coefs\n",
    "    def show_most_informative_features(vectorizer, clf, n=n):\n",
    "        feature_names = vectorizer.get_feature_names()\n",
    "        coefs_with_fns = sorted(zip(clf.coef_[0], feature_names), reverse=True)\n",
    "        top = zip(coefs_with_fns[:n], coefs_with_fns[:-(n + 1):-1])\n",
    "        print('Most Informative Features\\n(coef score, % of df has feature, feature string)\\n')\n",
    "        for (coef_1, fn_1), (coef_2, fn_2) in top:\n",
    "            p_1 = check_feature_perc(fn_1, df)\n",
    "            print('{:.3f} {:.0f}% {}'.format(coef_1, p_1, fn_1))\n",
    "            \n",
    "            #p_2 = check_feature_perc(fn_2, df)\n",
    "            #print('{:.0f}% {} {:.3f}\\t\\t{:.0f}% {} {:.3f}'.format(p_1, fn_1, coef_1, p_2, fn_2, coef_2))\n",
    "            # print('{:.3f} {}, {:.3f} {}'.format(coef_1, fn_1, coef_2, fn_2))\n",
    "\n",
    "    show_most_informative_features(v, mnb)\n",
    "    \n",
    "\n",
    "# call function\n",
    "get_features(initial_sample, n=20, transform='t', ngrams=(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding to stop words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "(coef score, % of df has feature, feature string)\n",
      "\n",
      "-10.796 33% original message\n",
      "-11.151 19% cc subject\n",
      "-11.160 0% let know\n",
      "-11.595 10% would like\n",
      "-11.659 9% sent monday\n",
      "-11.709 8% sent tuesday\n",
      "-11.740 8% please let\n",
      "-11.773 7% sally beck\n",
      "-11.773 0% please let know\n",
      "-11.777 8% sent thursday\n",
      "-11.851 6% october pm\n",
      "-11.876 7% sent wednesday\n",
      "-11.884 4% http www\n",
      "-11.894 8% subject fw\n",
      "-11.934 6% sent friday\n",
      "-11.959 5% north america\n",
      "-11.970 5% blair lynn\n",
      "-12.013 3% michelle cash\n",
      "-12.042 4% thanks lynn\n",
      "-12.060 0% pm cc\n"
     ]
    }
   ],
   "source": [
    "# create a new stop words list that includes 'enron', 'ect', 'hou', 'phillip allen', 'eric bass', 'arnold john'\n",
    "add_stops = ['enron', 'ect', 'hou', 'phillip', 'allen', 'eric', 'bass', 'arnold', 'john', 'com']\n",
    "new_stops = list(stop_words) + add_stops\n",
    "new_stops = set(new_stops)\n",
    "\n",
    "# call function\n",
    "get_features(initial_sample, n=20, transform='t', ngrams=(2,5), stop_words=new_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review email phrases\n",
    "Here, we're going to pass phrases back to the cossim calculator and return the % of emails that meet the cossim threshold, relative to the overall number of emails in the sample. We'll use a lower threshold to amplify the significance of each phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_by_phrases(s, n=10000, p=0.8):\n",
    "    \n",
    "    first_sample = cos_sim_this(df=df, p=p, n=n)\n",
    "    print('% of sample frame that meets cosim threshold: {:.0f}%\\n'.format(len(first_sample)*100/n))\n",
    "    \n",
    "    for phrase in s:\n",
    "        cond = df.p_body.str.contains(phrase)\n",
    "        df_filt = df[cond]\n",
    "        second_sample = cos_sim_this(df=df_filt, p=p, n=n)\n",
    "        \n",
    "        print(\"'{}' filter: {:.0f}%\".format(phrase, len(second_sample)*100/n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of sample frame that meets cosim threshold: 6%\n",
      "\n",
      "'cc subject' filter: 18%\n",
      "'original message' filter: 20%\n",
      "'forwarded by' filter: 20%\n",
      "'subject fw' filter: 35%\n"
     ]
    }
   ],
   "source": [
    "s = ['cc subject', 'original message', 'forwarded by', 'subject fw']\n",
    "cosine_by_phrases(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From a random sample of 10,000 emails pulled from our dataset, **35%** of the sample meets the cossim threshold when filtered by the phrase **'subject fw'** versus 6% returned by a control sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of sample frame that meets cosim threshold: 3%\n",
      "\n",
      "'cc subject' filter: 10%\n",
      "'original message' filter: 11%\n",
      "'forwarded by' filter: 12%\n",
      "'subject fw' filter: 29%\n"
     ]
    }
   ],
   "source": [
    "cosine_by_phrases(s, p=.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Even when we increase the cossim threshold, the phrase **'subject fw'** still returns **29%** meeting the cossim threshold, compared to only 3% from a control sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the impact of filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 filters: 4%\n",
      "Frame Size: 182823, B/G Ratio: 1.315\n",
      "None\n",
      "\n",
      "1 filter('subject fw'): 4%\n",
      "Frame Size: 169560, B/G Ratio: 1.303\n",
      "None\n",
      "\n",
      "1 filter('original message'): 3%\n",
      "Frame Size: 131472, B/G Ratio: 1.227\n",
      "None\n",
      "\n",
      "1 filter('forwarded by'): 4%\n",
      "Frame Size: 149053, B/G Ratio: 1.327\n",
      "None\n",
      "\n",
      "1 filter('cc subject'): 4%\n",
      "Frame Size: 144231, B/G Ratio: 1.321\n",
      "None\n",
      "\n",
      "Frame Size: 90085, B/G Ratio: 1.222\n",
      "None\n",
      "4 filters: 2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n = 20000\n",
    "p = .95\n",
    "\n",
    "# none filtered\n",
    "a_sample = cos_sim_this(df=df, p=p, n=n)\n",
    "print(\"0 filters: {:.0f}%\".format(len(a_sample)*100/n))\n",
    "print(monitor_label(df))\n",
    "print('')\n",
    "\n",
    "# check impact of removing 1 phrase\n",
    "cond = (df.p_body.str.contains('subject fw'))\n",
    "f_df = df[~cond]\n",
    "a_sample = cos_sim_this(df=f_df, p=p, n=n)\n",
    "print(\"1 filter('subject fw'): {:.0f}%\".format(len(a_sample)*100/n))\n",
    "print(monitor_label(f_df))\n",
    "print('')\n",
    "\n",
    "# check impact of removing 1 phrase\n",
    "cond = (df.p_body.str.contains('original message'))\n",
    "f_df = df[~cond]\n",
    "a_sample = cos_sim_this(df=f_df, p=p, n=n)\n",
    "print(\"1 filter('original message'): {:.0f}%\".format(len(a_sample)*100/n))\n",
    "print(monitor_label(f_df))\n",
    "print('')\n",
    "\n",
    "# check impact of removing 1 phrase\n",
    "cond = (df.p_body.str.contains('forwarded by'))\n",
    "f_df = df[~cond]\n",
    "a_sample = cos_sim_this(df=f_df, p=p, n=n)\n",
    "print(\"1 filter('forwarded by'): {:.0f}%\".format(len(a_sample)*100/n))\n",
    "print(monitor_label(f_df))\n",
    "print('')\n",
    "\n",
    "# check impact of removing 1 phrase\n",
    "cond = (df.p_body.str.contains('cc subject'))\n",
    "f_df = df[~cond]\n",
    "a_sample = cos_sim_this(df=f_df, p=p, n=n)\n",
    "print(\"1 filter('cc subject'): {:.0f}%\".format(len(a_sample)*100/n))\n",
    "print(monitor_label(f_df))\n",
    "print('')\n",
    "\n",
    "# check impact of removing all phrases\n",
    "cond = (df.p_body.str.contains('subject fw') | df.p_body.str.contains('forwarded by') | df.p_body.str.contains('original message') | df.p_body.str.contains('cc subject'))\n",
    "f_df = df[~cond]\n",
    "a_sample = cos_sim_this(df=f_df, p=p, n=n)\n",
    "print(monitor_label(f_df))\n",
    "print(\"4 filters: {:.0f}%\\n\".format(len(a_sample)*100/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Though **subject fw** has the higher similarity scores when isolated, the frequency of the phrase across the dataset is low - mitigating the influence of possible duplicates.    \n",
    "> \n",
    "> When we apply each filter individually, there isn't a significant reduction in the cos sim score, relative to the reduction across the dataset. When we apply all 4 filters, both the dataset size and cosim % are reduced roughly the same amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./data/enron/03_filtered_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
