{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 TF-IDF to Clean Email Bodies\n",
    "- Preprocess\n",
    "- Calculate TF-IDF\n",
    "- Ranking with Cosine Similarity\n",
    "\n",
    "## Preprocessing\n",
    "To preprocess the emails, let's create a collection of functions that will perform each process on a body of text. Then, we can tie them together in an umbrella function, and apply it to the series with `series.apply()`.\n",
    "\n",
    "- 0.1 Isolate key, body in subFrame\n",
    "- 0.2 Convert to lowercase\n",
    "- 0.3 Remove stop words (nltk library)\n",
    "- 0.4 Remove punctuation\n",
    " - *special case: \" ' \"*\n",
    "- 0.5 Single characters\n",
    "- 0.6 Stemming (Stemming converts words to its stem) AND/OR Lemmatisation (Reduce word to root synonym of a word; will make sure is dictionary word)\n",
    " - Can do either or both; recommend lemma then stem\n",
    "- 0.7 Converting numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import master DataFrame mdf\n",
    "df = pd.read_csv('./data/02_add_gender.csv', header=0, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 169285 entries, 0 to 169284\n",
      "Data columns (total 21 columns):\n",
      "f_dir        169285 non-null object\n",
      "m_id         169285 non-null object\n",
      "m_date       169285 non-null object\n",
      "m_from       169285 non-null object\n",
      "m_to         163413 non-null object\n",
      "m_cc         52714 non-null object\n",
      "m_bcc        50250 non-null object\n",
      "m_subj       162429 non-null object\n",
      "mime_vers    169285 non-null float64\n",
      "cont_type    169285 non-null object\n",
      "encode       169285 non-null object\n",
      "x_from       169285 non-null object\n",
      "x_to         164697 non-null object\n",
      "x_cc         50633 non-null object\n",
      "x_bcc        131 non-null object\n",
      "x_fold       169285 non-null object\n",
      "x_orig       169285 non-null object\n",
      "x_fname      167315 non-null object\n",
      "m_body       169285 non-null object\n",
      "name         162847 non-null object\n",
      "gender       162847 non-null object\n",
      "dtypes: float64(1), object(20)\n",
      "memory usage: 27.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters across all email bodies in corpus: 135952755.\n"
     ]
    }
   ],
   "source": [
    "# total email body chars (for process tracking)\n",
    "start_chars = df.m_body.apply(len).sum()\n",
    "\n",
    "print('Total characters across all email bodies in corpus: {}.'.format(start_chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f_dir</th>\n",
       "      <th>m_id</th>\n",
       "      <th>m_date</th>\n",
       "      <th>m_from</th>\n",
       "      <th>m_to</th>\n",
       "      <th>m_cc</th>\n",
       "      <th>m_bcc</th>\n",
       "      <th>m_subj</th>\n",
       "      <th>mime_vers</th>\n",
       "      <th>cont_type</th>\n",
       "      <th>...</th>\n",
       "      <th>x_from</th>\n",
       "      <th>x_to</th>\n",
       "      <th>x_cc</th>\n",
       "      <th>x_bcc</th>\n",
       "      <th>x_fold</th>\n",
       "      <th>x_orig</th>\n",
       "      <th>x_fname</th>\n",
       "      <th>m_body</th>\n",
       "      <th>name</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>allen-p/_sent_mail/1</td>\n",
       "      <td>&lt;18782981.1075855378110.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Mon, 14 May 2001 16:39:00 -0700 (PDT)</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>tim.belden@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>...</td>\n",
       "      <td>Phillip K Allen</td>\n",
       "      <td>Tim Belden &lt;Tim Belden/Enron@EnronXGate&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...</td>\n",
       "      <td>Allen-P</td>\n",
       "      <td>pallen (Non-Privileged).pst</td>\n",
       "      <td>Here is our forecast</td>\n",
       "      <td>phillip</td>\n",
       "      <td>boy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allen-p/_sent_mail/10</td>\n",
       "      <td>&lt;15464986.1075855378456.JavaMail.evans@thyme&gt;</td>\n",
       "      <td>Fri, 4 May 2001 13:51:00 -0700 (PDT)</td>\n",
       "      <td>phillip.allen@enron.com</td>\n",
       "      <td>john.lavorato@enron.com</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Re:</td>\n",
       "      <td>1.0</td>\n",
       "      <td>text/plain; charset=us-ascii</td>\n",
       "      <td>...</td>\n",
       "      <td>Phillip K Allen</td>\n",
       "      <td>John J Lavorato &lt;John J Lavorato/ENRON@enronXg...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...</td>\n",
       "      <td>Allen-P</td>\n",
       "      <td>pallen (Non-Privileged).pst</td>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "      <td>phillip</td>\n",
       "      <td>boy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   f_dir                                           m_id  \\\n",
       "0   allen-p/_sent_mail/1  <18782981.1075855378110.JavaMail.evans@thyme>   \n",
       "1  allen-p/_sent_mail/10  <15464986.1075855378456.JavaMail.evans@thyme>   \n",
       "\n",
       "                                  m_date                   m_from  \\\n",
       "0  Mon, 14 May 2001 16:39:00 -0700 (PDT)  phillip.allen@enron.com   \n",
       "1   Fri, 4 May 2001 13:51:00 -0700 (PDT)  phillip.allen@enron.com   \n",
       "\n",
       "                      m_to m_cc m_bcc m_subj  mime_vers  \\\n",
       "0     tim.belden@enron.com  NaN   NaN    NaN        1.0   \n",
       "1  john.lavorato@enron.com  NaN   NaN    Re:        1.0   \n",
       "\n",
       "                      cont_type  ...           x_from  \\\n",
       "0  text/plain; charset=us-ascii  ...  Phillip K Allen   \n",
       "1  text/plain; charset=us-ascii  ...  Phillip K Allen   \n",
       "\n",
       "                                                x_to x_cc x_bcc  \\\n",
       "0           Tim Belden <Tim Belden/Enron@EnronXGate>  NaN   NaN   \n",
       "1  John J Lavorato <John J Lavorato/ENRON@enronXg...  NaN   NaN   \n",
       "\n",
       "                                              x_fold   x_orig  \\\n",
       "0  \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...  Allen-P   \n",
       "1  \\Phillip_Allen_Jan2002_1\\Allen, Phillip K.\\'Se...  Allen-P   \n",
       "\n",
       "                       x_fname  \\\n",
       "0  pallen (Non-Privileged).pst   \n",
       "1  pallen (Non-Privileged).pst   \n",
       "\n",
       "                                              m_body     name gender  \n",
       "0                               Here is our forecast  phillip    boy  \n",
       "1  Traveling to have a business meeting takes the...  phillip    boy  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Set on Gender Null Values, Not Found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169285"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender\n",
       "boy          92565\n",
       "girl         69849\n",
       "not_found      433\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check gender values\n",
    "df.groupby(['gender']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6438"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null count\n",
    "len(df[df.gender.isnull()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create capture condition: is null OR gender not found\n",
    "cond = (df.gender.isnull()) | (df.gender == 'not_found')\n",
    "\n",
    "# remove condition\n",
    "df = df[~cond]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Set Based on Character Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Isolate key, body in subFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create subFrame with directory, email body text\n",
    "df = df[['m_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here is our forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Traveling to have a business meeting takes the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test successful.  way to go!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Randy,   Can you send me a schedule of the sal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Let's shoot for Tuesday at 11:45.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              m_body\n",
       "0                               Here is our forecast\n",
       "1  Traveling to have a business meeting takes the...\n",
       "2                     test successful.  way to go!!!\n",
       "3  Randy,   Can you send me a schedule of the sal...\n",
       "4                  Let's shoot for Tuesday at 11:45."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create testFrame from subFrame for method testing\n",
    "#df = df.sample(n=5000, random_state=1)\n",
    "#df = df.reset_index()\n",
    "#tsf = sf.copy()\n",
    "\n",
    "# show testFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                 here is our forecast\n",
      "1    traveling to have a business meeting takes the...\n",
      "2                       test successful.  way to go!!!\n",
      "3    randy,   can you send me a schedule of the sal...\n",
      "4                    let's shoot for tuesday at 11:45.\n",
      "Name: m_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# lowers case\n",
    "def no_talls(t):\n",
    "\n",
    "    return t.lower()\n",
    "\n",
    "# test\n",
    "print(df.m_body.head().apply(no_talls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3 Remove stop words (nltk library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import nltk # natural language toolkit\n",
    "from nltk.tokenize import word_tokenize # creates tokenized words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# set stopwords list to english\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                        Here forecast\n",
      "1     Traveling business meeting takes fun trip . E...\n",
      "2                       test successful . way go ! ! !\n",
      "3     Randy , Can send schedule salary level everyo...\n",
      "4                         Let 's shoot Tuesday 11:45 .\n",
      "Name: m_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# removes stopwords\n",
    "def no_stops(t):\n",
    "    \n",
    "    # new text to be returned\n",
    "    nt = \"\"\n",
    "    \n",
    "    # split text into list of words\n",
    "    words = word_tokenize(t)\n",
    "    \n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            nt = nt + ' ' + word\n",
    "    \n",
    "    return nt\n",
    "    \n",
    "# test\n",
    "print(df.m_body.head().apply(no_stops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4 Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                 Here is our forecast\n",
      "1    Traveling to have a business meeting takes the...\n",
      "2                       test successful   way to go   \n",
      "3    Randy    Can you send me a schedule of the sal...\n",
      "4                    Let's shoot for Tuesday at 11 45 \n",
      "Name: m_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# removes select punctuation\n",
    "def no_puncs(t):\n",
    "    \n",
    "    # list of punctuation marks\n",
    "    puncs = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n,\"\n",
    "\n",
    "    for p in puncs:\n",
    "        t = t.replace(p, ' ')\n",
    "    \n",
    "    return t\n",
    "\n",
    "# test\n",
    "print(df.m_body.head().apply(no_puncs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4.1 Remove apostrophe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                 Here is our forecast\n",
      "1    Traveling to have a business meeting takes the...\n",
      "2                       test successful.  way to go!!!\n",
      "3    Randy,   Can you send me a schedule of the sal...\n",
      "4                    Let s shoot for Tuesday at 11:45.\n",
      "Name: m_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# removes apostrophe\n",
    "def no_aposts(t):\n",
    "    \n",
    "    t = t.replace(\"'\", ' ')\n",
    "    \n",
    "    return t\n",
    "\n",
    "# test\n",
    "print(df.m_body.head().apply(no_aposts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.5 Single characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                 Here is our forecast\n",
      "1     Traveling to have business meeting takes the ...\n",
      "2                            test successful way to go\n",
      "3     Randy Can you send me schedule of the salary ...\n",
      "4                    Let 's shoot for Tuesday at 11:45\n",
      "Name: m_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# remove single characters\n",
    "def no_singles(t):\n",
    "    \n",
    "    nt = \"\"\n",
    "    \n",
    "    words = word_tokenize(t)\n",
    "    \n",
    "    for word in words:\n",
    "        if len(word) > 1:\n",
    "            nt = nt + ' ' + word\n",
    "            \n",
    "    return nt\n",
    "\n",
    "# test\n",
    "print(df.m_body.head().apply(no_singles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.6.1 Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# create instance\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# lemmatizer test\n",
    "print(lemmatizer.lemmatize('better', pos = 'a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                 Here be our forecast\n",
      "1     Traveling to have a business meet take the fu...\n",
      "2                    test successful . way to go ! ! !\n",
      "3     Randy , Can you send me a schedule of the sal...\n",
      "4                  Let 's shoot for Tuesday at 11:45 .\n",
      "Name: m_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# lemmatize text\n",
    "def go_lemms(t):\n",
    "    \n",
    "    nt = \"\"\n",
    "    \n",
    "    words = word_tokenize(t)\n",
    "    \n",
    "    for word in words:\n",
    "        word = lemmatizer.lemmatize(word) # noun\n",
    "        word = lemmatizer.lemmatize(word, pos = 'a') # adjective\n",
    "        word = lemmatizer.lemmatize(word, pos = 'v') # verb\n",
    "        nt = nt + ' ' + word\n",
    "\n",
    "    return nt\n",
    "\n",
    "# test\n",
    "print(df.m_body.head().apply(go_lemms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.6.2 Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "# import library\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# create an instance\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# stemmer test\n",
    "print(stemmer.stem(\"running\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                 here is our forecast\n",
      "1     travel to have a busi meet take the fun out o...\n",
      "2                       test success . way to go ! ! !\n",
      "3     randi , can you send me a schedul of the sala...\n",
      "4                  let 's shoot for tuesday at 11:45 .\n",
      "Name: m_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# stem text\n",
    "def go_stems(t):\n",
    "\n",
    "    nt = \"\"\n",
    "    \n",
    "    words = word_tokenize(t)\n",
    "\n",
    "    for word in words:\n",
    "        nt = nt + ' ' + stemmer.stem(word)\n",
    "\n",
    "    return nt\n",
    "        \n",
    "# test\n",
    "print(df.m_body.head().apply(go_stems))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.7 Converting numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from num2words import num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                 Here is our forecast\n",
      "1     Traveling to have a business meeting takes th...\n",
      "2                    test successful . way to go ! ! !\n",
      "3     Randy , Can you send me a schedule of the sal...\n",
      "4                  Let 's shoot for Tuesday at 11:45 .\n",
      "Name: m_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# convert numbers to text\n",
    "def no_numbs(t):\n",
    "        \n",
    "    nt = \"\"\n",
    "    \n",
    "    words = word_tokenize(t)\n",
    "\n",
    "    for word in words:\n",
    "        try:\n",
    "            word = num2words(int(word))\n",
    "        except:\n",
    "            a = 0\n",
    "        nt = nt + ' ' + word\n",
    "\n",
    "    return nt\n",
    "\n",
    "# test\n",
    "print(df.m_body.head().apply(no_numbs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.8 Preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                             forecast\n",
      "1     travel busi meet take fun trip especi prepar ...\n",
      "2                                  test success way go\n",
      "3     randi send schedul salari level everyon sched...\n",
      "4                     let shoot tuesday eleven forty f\n",
      "Name: m_body, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# combines preprocessing steps\n",
    "def preprocess(t):\n",
    "\n",
    "    t = no_talls(t)\n",
    "    t = no_puncs(t)    \n",
    "    t = no_aposts(t)\n",
    "    t = no_singles(t)\n",
    "    t = no_numbs(t)\n",
    "    t = no_stops(t)\n",
    "    t = go_lemms(t)\n",
    "    t = go_stems(t)\n",
    "    t = no_puncs(t)    \n",
    "    t = no_numbs(t)\n",
    "    \n",
    "    # process numbers to text\n",
    "    #t = no_talls(t)\n",
    "    #t = no_numbs(t)\n",
    "\n",
    "    # process punctuation, single characters, stopwords\n",
    "    #t = no_puncs(t)\n",
    "    #t = no_aposts(t)    \n",
    "    #t = no_singles(t) \n",
    "    #t = no_stops(t)\n",
    "\n",
    "    # t = no_numbs(t)\n",
    "    \n",
    "    # lemm & stem\n",
    "    #t = go_lemms(t)\n",
    "    #t = go_stems(t) \n",
    "\n",
    "    # cleanup set\n",
    "    #t = no_numbs(t)\n",
    "\n",
    "    # return final text\n",
    "    return t\n",
    "\n",
    "# test\n",
    "print(df.m_body.head().apply(preprocess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Sample Into New Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 32s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>m_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>forecast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>travel busi meet take fun trip especi prepar ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test success way go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>randi send schedul salari level everyon sched...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>let shoot tuesday eleven forty f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              m_body\n",
       "0                                           forecast\n",
       "1   travel busi meet take fun trip especi prepar ...\n",
       "2                                test success way go\n",
       "3   randi send schedul salari level everyon sched...\n",
       "4                   let shoot tuesday eleven forty f"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# create new column\n",
    "df['m_body'] = df.m_body.apply(preprocess)\n",
    "\n",
    "# concurrent futures executor\n",
    "#with concurrent.futures.ThreadPoolExecutor() as executor:  \n",
    "    #tsf['p_body'] = executor.map(preprocess, tsf.m_body)\n",
    "\n",
    "# view head\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters after processing: 81459576.\n"
     ]
    }
   ],
   "source": [
    "# total email body chars (for process tracking)\n",
    "process_chars = df.m_body.apply(len).sum()\n",
    "\n",
    "print('Total characters after processing: {}.'.format(process_chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Any Blanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #check count of entries\n",
    "    print('DataFrame Entries: {}.'.format(len(df)))\n",
    "\n",
    "    # condition set to blank body\n",
    "    cond = df.m_body == ''\n",
    "\n",
    "    print(df[cond])\n",
    "\n",
    "    # return non-blanks to new df\n",
    "    df = df[~cond]\n",
    "\n",
    "    #check count of entries\n",
    "    print('DataFrame Entries: {}.'.format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating TF-IDF\n",
    "- Calculate *document frequency*\n",
    "- Calculate TF-IDF\n",
    "\n",
    "### 1.0 Calculate DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through all words in all documents, store document id's for each word\n",
    "\n",
    "def calculate_doc_freq(text_body, a_dict):\n",
    "    \n",
    "    words = word_tokenize(text_body)\n",
    "    \n",
    "    for i, w in enumerate(words):\n",
    "        \n",
    "        try:\n",
    "            a_dict[w].add(i)\n",
    "            \n",
    "        except:\n",
    "            a_dict[w] = {i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 111867 unique words.\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DF_dict = {} # dictionary for document frequency (ie occurence of word across all documents)\n",
    "\n",
    "# call document frequency method\n",
    "df.m_body.apply(calculate_doc_freq, a_dict=DF_dict)\n",
    "\n",
    "# prints number of words found\n",
    "print('Found {} unique words.'.format(len(DF_dict)))\n",
    "\n",
    "# replace the set of catches with a count\n",
    "for i in DF_dict:\n",
    "    DF_dict[i] = len(DF_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['forecast', 'travel', 'busi', 'meet', 'take']\n"
     ]
    }
   ],
   "source": [
    "# count of total words included in the corpus\n",
    "total_vocab_size = len(DF_dict)\n",
    "\n",
    "# creates list of total vocab\n",
    "total_vocab = [word for word in DF_dict]\n",
    "\n",
    "# view list\n",
    "print(total_vocab[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Calcuate TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents in current corpus is 162414.\n"
     ]
    }
   ],
   "source": [
    "# get number of corpus\n",
    "N = len(df)\n",
    "\n",
    "# print text\n",
    "print('The number of documents in current corpus is {}.'.format(N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the document frequency of a word\n",
    "def doc_freq(word):\n",
    "\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF_dict[word]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates tf_idf\n",
    "def calculate_tf_idf(doc_id, text_body):\n",
    "    \"\"\"Calculates tf-idf and saves the score to the tf_idf dictionary as a (doc_id, word) tuple key.\n",
    "       \n",
    "       Make sure to initialize the 'tf_idf' dictionary outside of this function prior to call.\n",
    "    \n",
    "       Arguments: \n",
    "           - Document identifier\n",
    "           - Body of text\"\"\"\n",
    "    \n",
    "    # tokenizes each word in the text body\n",
    "    words = word_tokenize(text_body)\n",
    "    \n",
    "    # creates a collection of word counts from the text body; referenced like a dictionary\n",
    "    counter = collections.Counter(words)\n",
    "    \n",
    "    # loop through each word in list using unique values\n",
    "    for word in np.unique(words):\n",
    "        \n",
    "        # term frequency = count of word in document / number of words in document\n",
    "        tf = counter[word]/len(words)\n",
    "        \n",
    "        # document frequency is called from df dictionary\n",
    "        df = doc_freq(word)\n",
    "        \n",
    "        # inverse document frequency is calculated with log(N as number of documents in corpus / df + 1)\n",
    "        # 1 is added to df during inverse incase no words exist so that a #div0 is not returned\n",
    "        idf = math.log(N/(df+1))\n",
    "        \n",
    "        # record tf-idf score to doc_id, word key\n",
    "        tf_idf[doc_id, word] = tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# initalize dictionary\n",
    "tf_idf = {}\n",
    "\n",
    "# loop through index and processed text items\n",
    "for idx, email in df.m_body.items():\n",
    "    \n",
    "    # calculate tf-idf\n",
    "    calculate_tf_idf(idx, email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7117964\n"
     ]
    }
   ],
   "source": [
    "# number of tf_idf scores\n",
    "print(len(tf_idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking with Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates cosine similarity between two text bodies\n",
    "def cosine_sim(a, b):\n",
    "    \"\"\"Calculates cosine similarity between two text bodies.\n",
    "       Arguments:\n",
    "       - a: set of vectorized values\n",
    "       - b: second body vectorized set\"\"\"\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing tf-idf Across Emails, Total Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector(text_body):\n",
    "    \n",
    "    # tokenizes each word in the text body\n",
    "    words = word_tokenize(text_body)\n",
    "    \n",
    "    Q = np.zeros((total_vocab_size))\n",
    "    \n",
    "    counter = collections.Counter(words)\n",
    "    words_count = len(words)\n",
    "    \n",
    "    for word in np.unique(words):\n",
    "        \n",
    "        tf = counter[word]/words_count\n",
    "        df = doc_freq(word)\n",
    "        idf = math.log((N)/(df+1))\n",
    "        \n",
    "        try:\n",
    "            idx = total_vocab.index(word)\n",
    "            Q[idx] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim_bulk(corpus, query, q_val):\n",
    "    \"\"\"Returns a dictionary of any cosine similarity calculations that match query.\n",
    "       Note: All cosine similarity are calculated within the function. \n",
    "             For accurate returns, the index should match the sequence of the corpus entries.\n",
    "       Arguments:\n",
    "       corpus: a collection of text bodies\n",
    "       query: either of three  is >, <, =\n",
    "       q_val is the value to query.\"\"\"\n",
    "\n",
    "    # initialize dictionary\n",
    "    #a_dict = {}\n",
    "    \n",
    "    # get length of corpus\n",
    "    c = len(corpus)\n",
    "    \n",
    "    # setup the loop cycle\n",
    "    for a in range(c):\n",
    "        av = gen_vector(df.iloc[a, 0])\n",
    "        for b in range(c):\n",
    "            # reduce output by not including 1 CS match to self\n",
    "            if a == b:\n",
    "                pass\n",
    "            else:\n",
    "                bv = gen_vector(df.iloc[b, 0])\n",
    "\n",
    "                # calculate cosine similarity\n",
    "                score = cosine_sim(av, bv)\n",
    "\n",
    "                # evaluate the query\n",
    "                if query == 'g':\n",
    "                    if (score > q_val):\n",
    "                        try:\n",
    "                            CS_dict[a].add(b)\n",
    "                        except:\n",
    "                            CS_dict[a] = {b}\n",
    "                # if less than\n",
    "                elif query == 'l':\n",
    "                    if (score < q_val):\n",
    "                        try:\n",
    "                            CS_dict[a].add(b)\n",
    "                        except:\n",
    "                            CS_dict[a] = {b}\n",
    "                # if equal to\n",
    "                elif query == 'e':\n",
    "                    if (score == q_val):\n",
    "                        try:\n",
    "                            CS_dict[a].add(b)\n",
    "                        except:\n",
    "                            CS_dict[a] = {b}\n",
    "    \n",
    "    #return a_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# return CS \n",
    "\n",
    "CS_dict = {}\n",
    "\n",
    "test_frame = list(df.m_body)\n",
    "\n",
    "corpus = test_frame\n",
    "query = 'g'\n",
    "q_val = .95\n",
    "\n",
    "cos_sim_bulk(corpus, query, q_val)\n",
    "print(len(CS_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(k, doc_id, text_body):\n",
    "    \n",
    "    # tokenizes each word in the text body\n",
    "    # words = word_tokenize(text_body)\n",
    "    \n",
    "    print('Cosine Similarity')\n",
    "    \n",
    "    print('\\nQuery: ', doc_id)\n",
    "\n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(text_body)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "    \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    \n",
    "    print('')\n",
    "    print(out)\n",
    "    print('')\n",
    "    \n",
    "    d_cosines.sort(reverse = True)\n",
    "    \n",
    "    for thing in d_cosines[:k]:\n",
    "        print(thing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CS_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: {197}, 14: {198}, 19: {199}, 25: {200}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(CS_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' travel busi meet take fun trip especi prepar present would suggest hold busi plan meet take trip without formal busi meet would even tri get honest opinion whether trip even desir necessari far busi meet think would product tri stimul discuss across differ group work often present speak other quiet wait turn meet might good hold round tabl discuss format suggest go austin play golf rent ski boat jet ski fli somewher take much time'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST CELL check body text\n",
    "df.loc[1, 'm_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' travel busi meet take fun trip especi prepar present would suggest hold busi plan meet take trip without formal busi meet would even tri get honest opinion whether trip even desir necessari far busi meet think would product tri stimul discuss across differ group work often present speak other quiet wait turn meet might good hold round tabl discuss format suggest go austin play golf rent ski boat jet ski fli somewher take much time'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST CELL check body text\n",
    "df.loc[197, 'm_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' reagan want give updat chang unit mix includ bedroom reduc number build twelv kipp flore work construct draw time pursu fha financ construct draw complet send revis bid origin bid competit still attract firm strong local presenc contact phillip'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST CELL check body text\n",
    "df.loc[19, 'm_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' reagan want give updat chang unit mix includ bedroom reduc number build twelv kipp flore work construct draw time pursu fha financ construct draw complet send revis bid origin bid competit still attract firm strong local presenc contact phillip'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST CELL check body text\n",
    "df.loc[199, 'm_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
